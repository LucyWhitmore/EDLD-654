---
title: "Models"
author: "Lucy Whitmore"
date: "11/30/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Load data
```{r}
brain_data <- rio::import("brain_data.Rda")
```

# Split into train/test
```{r}
# Initial data preparation

require(caret)
require(recipes)

set.seed(10152021)  # for reproducibility

# Train/Test Split
  
loc      <- sample(1:nrow(brain_data), round(nrow(brain_data) * 0.8))
brain_tr  <- brain_data[loc, ]
brain_te  <- brain_data[-loc, ]

```

## Blueprint 
```{r}
require(recipes)

blueprint <- recipe(x     = brain_data,
                    vars  = colnames(brain_data),
                    roles = c('outcome',rep('predictor',172))) %>%  # change this
             step_zv(all_numeric()) %>%
             step_nzv(all_numeric()) %>%
             step_normalize(all_numeric_predictors())

```

## Cross-Validation
```{r}
set.seed(10152021)  # for reproducibility
# Randomly shuffle the data
    brain_tr = brain_tr[sample(nrow(brain_tr)),]

# Create 10 folds with equal size
    folds = cut(seq(1,nrow(brain_tr)),breaks=10,labels=FALSE)
  
# Create the list for each fold 
    my.indices <- vector('list',10)
    for(i in 1:10){
        my.indices[[i]] <- which(folds!=i)
    }
      
cv <- trainControl(method = "cv",
                   index  = my.indices)


```


## Unregularized Linear Regression
```{r}

caret_mod <- caret::train(blueprint, 
                          data      = brain_tr, 
                          method    = "lm", 
                          trControl = cv)

caret_mod

```

```{r}
save(caret_mod, file="caret_mod.Rda")

```

```{r}
predicted_te <- predict(caret_mod, brain_te)

prediction_actual <- cbind(as.data.frame(predicted_te), brain_te %>% select(interview_age))

save(prediction_actual, file="prediction_actual.Rda")

rsq_te <- cor(brain_te$interview_age,predicted_te)^2   # change target
rsq_te

mae_te <- mean(abs(brain_te$interview_age - predicted_te))
mae_te

rmse_te <- sqrt(mean((brain_te$interview_age - predicted_te)^2))
rmse_te

```

```{r}
require(vip)

vip(caret_mod, 
    num_features = 10, 
    geom = "point") + 
theme_bw()

```


## LASSO Regression

```{r}
#grid <- data.frame(alpha = 1, lambda = seq(0.01,3,.01)) 
grid <- data.frame(alpha = 1, lambda = seq(0,0.015,.001)) 

grid

#Note. Remember how glmnet multiplies the lambda by sample size (N). In this case, the sample size is 2834. So, for instance a lambda value of 1 would be 2834. You can try larger values and explore, but in this case a max value of 3 for lambda would be more than enough. I don't think it will improve performance beyond this value

lasso <- caret::train(blueprint, 
                        data      = brain_tr, 
                        method    = "glmnet", 
                        family    = "gaussian", 
                        trControl = cv,
                        tuneGrid  = grid)
```

```{r}
lasso$bestTune

require(vip)

vip(lasso, 
    num_features = 10, 
    geom = "point") + 
theme_bw()
```

```{r}
# Predict
predict_te_lasso <- predict(lasso, brain_te)

# Evaluate performace
rsq_te_lasso <- cor(brain_te$interview_age,predict_te_lasso)^2   # change target
rsq_te_lasso

mae_te_lasso <- mean(abs(brain_te$interview_age - predict_te_lasso))
mae_te_lasso

rmse_te_lasso <- sqrt(mean((brain_te$interview_age - predict_te_lasso)^2))
rmse_te_lasso

```

## Bagged Tree
```{r}
# Grid, running with all predictors in the data (768)

grid_bag <- expand.grid(mtry = 172, splitrule='variance',min.node.size=2)  # change mtry
grid_bag

```

```{r}
# Run the bagged trees by iterating over num.trees using the 
# values 5, 20, 40, 60,  ..., 200
  
  nbags <- c(5,seq(from = 20,to = 200, by = 20))
    
  bags <- vector('list',length(nbags))
    
    for(i in 1:length(nbags)){
      
      bags[[i]] <- caret::train(blueprint,
                                data      = brain_tr,
                                method    = 'ranger',
                                trControl = cv,
                                tuneGrid  = grid_bag,
                                num.trees = nbags[i],
                                max.depth = 60)   # can try to increase max depth
      
      print(i)
      
    }

    # This can take a few hours to run.

```

```{r}

rmses <- c()

for(i in 1:length(nbags)){
  
  rmses[i] = bags[[i]]$results$RMSE
  
}

nbags[which.min(rmses)]




predicted_te_bag <- predict(bags[[11]], brain_te) # use whichever is best

# MAE
mae_te_bag <- mean(abs(brain_te$interview_age - predicted_te_bag))
mae_te_bag 
 
# RMSE
rmse_te_bag <- sqrt(mean((brain_te$interview_age - predicted_te_bag)^2))
rmse_te_bag

# R-square
rsq_te_bag <- cor(brain_te$interview_age,predicted_te_bag)^2
rsq_te_bag
```


Table of performance metrics
```{r}



mae_te<- .79
rmse_te<- .97
rsq_te <- .39

mae_te_lasso<- .79
rmse_te_lasso<- .97
rsq_te_lasso <- .36

mae_te_bag <- .88
rmse_te_bag <- 1.08
rsq_te_bag <- .22

performance <- data.frame(Model = c("Logistic Regression", "Logistic Regression with LASSO Penalty", "Bagged Trees"),
                 RSQ = c(rsq_te, rsq_te_lasso, rsq_te_bag),
                 MAE = c(mae_te, mae_te_lasso, mae_te_bag),
                 RMSE = c(rmse_te, rmse_te_lasso, rmse_te_bag))

performance

```
